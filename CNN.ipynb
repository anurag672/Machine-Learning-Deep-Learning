{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up-y09IlFRit"
   },
   "source": [
    "# 1 importing libraries\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "189vRh2qCPn-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u5rBvw0F7s8"
   },
   "source": [
    "# 2 Data Preprocessing\n",
    "\n",
    "# 2.1 Processing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "crHFVDPlF7Di",
    "outputId": "a7aa2788-9c89-40b8-8d63-b95478c2acc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_O7r6OBHmX8"
   },
   "source": [
    "# 2.2 Processing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "76dMYyJfHrff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWmtae8OTqBZ"
   },
   "source": [
    "# 3. Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4lm0FPrVT2hv"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWRAcScKUGSo"
   },
   "source": [
    "# 3.1 Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1_mrzmaiUZ2m"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation= 'relu', input_shape = [64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1teIMX7WnS2"
   },
   "source": [
    "# 3.2 Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lFN-mxBZWq7Q"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr22-9WNX3k5"
   },
   "source": [
    "# 3.3 Adding one more convolution and pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "L6EwkOcNX8Yv"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation= 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnzztQLnYDF4"
   },
   "source": [
    "# 3.4 Flattering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QUnLcWL2YIVL"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiBUCjz1YXyc"
   },
   "source": [
    "# 4. Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AQ0ZDIjlYb7S"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXef7rUOY-_0"
   },
   "source": [
    "# 5. Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q-ARC9s0ZBkg"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35Eet9BEaFdc"
   },
   "source": [
    "# 6. Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tLWTl8B7aIVu"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z86EHKabE17"
   },
   "source": [
    "# 7. training the cnn on training_set and evaluation on the test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VHOB_CwLbLSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 193s 773ms/step - loss: 0.6799 - accuracy: 0.5659 - val_loss: 0.6352 - val_accuracy: 0.6595\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 74s 296ms/step - loss: 0.6127 - accuracy: 0.6671 - val_loss: 0.5824 - val_accuracy: 0.6985\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 73s 292ms/step - loss: 0.5726 - accuracy: 0.7004 - val_loss: 0.5395 - val_accuracy: 0.7365\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 73s 293ms/step - loss: 0.5305 - accuracy: 0.7256 - val_loss: 0.5120 - val_accuracy: 0.7545\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 73s 292ms/step - loss: 0.5067 - accuracy: 0.7530 - val_loss: 0.4834 - val_accuracy: 0.7760\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 73s 292ms/step - loss: 0.4771 - accuracy: 0.7694 - val_loss: 0.4822 - val_accuracy: 0.7705\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 75s 302ms/step - loss: 0.4691 - accuracy: 0.7786 - val_loss: 0.4776 - val_accuracy: 0.7775\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 73s 293ms/step - loss: 0.4481 - accuracy: 0.7878 - val_loss: 0.4754 - val_accuracy: 0.7810\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.4406 - accuracy: 0.7965 - val_loss: 0.4825 - val_accuracy: 0.7680\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 75s 300ms/step - loss: 0.4380 - accuracy: 0.7933 - val_loss: 0.4664 - val_accuracy: 0.7870\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 73s 294ms/step - loss: 0.4210 - accuracy: 0.8026 - val_loss: 0.5005 - val_accuracy: 0.7655\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.4080 - accuracy: 0.8141 - val_loss: 0.4445 - val_accuracy: 0.7940\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.4027 - accuracy: 0.8158 - val_loss: 0.4578 - val_accuracy: 0.8010\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 74s 296ms/step - loss: 0.3961 - accuracy: 0.8186 - val_loss: 0.4336 - val_accuracy: 0.8025\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 73s 294ms/step - loss: 0.3822 - accuracy: 0.8270 - val_loss: 0.4523 - val_accuracy: 0.7940\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.3769 - accuracy: 0.8321 - val_loss: 0.4743 - val_accuracy: 0.7925\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3707 - accuracy: 0.8298 - val_loss: 0.4396 - val_accuracy: 0.8080\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3541 - accuracy: 0.8434 - val_loss: 0.4388 - val_accuracy: 0.8080\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3525 - accuracy: 0.8465 - val_loss: 0.4578 - val_accuracy: 0.7985\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.3435 - accuracy: 0.8474 - val_loss: 0.4345 - val_accuracy: 0.8115\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3401 - accuracy: 0.8484 - val_loss: 0.4691 - val_accuracy: 0.7975\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3315 - accuracy: 0.8544 - val_loss: 0.4460 - val_accuracy: 0.8075\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.3210 - accuracy: 0.8566 - val_loss: 0.4467 - val_accuracy: 0.8120\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 73s 294ms/step - loss: 0.3282 - accuracy: 0.8522 - val_loss: 0.4573 - val_accuracy: 0.8170\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3145 - accuracy: 0.8624 - val_loss: 0.4439 - val_accuracy: 0.8135\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 73s 294ms/step - loss: 0.3043 - accuracy: 0.8649 - val_loss: 0.4690 - val_accuracy: 0.8165\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 73s 292ms/step - loss: 0.3023 - accuracy: 0.8705 - val_loss: 0.5106 - val_accuracy: 0.7890\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.2841 - accuracy: 0.8734 - val_loss: 0.5235 - val_accuracy: 0.7960\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 74s 296ms/step - loss: 0.2793 - accuracy: 0.8775 - val_loss: 0.4847 - val_accuracy: 0.8020\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.2705 - accuracy: 0.8814 - val_loss: 0.4656 - val_accuracy: 0.8135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x151c1c97940>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x= training_set, validation_data= test_set, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxNIHn7LbbjD"
   },
   "source": [
    "# 8. Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "M1yQjXE1AUFF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis =0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
